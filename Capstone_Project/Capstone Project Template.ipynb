{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# COVID-19 Daily Tracking Analysis\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project will be building a data warehouse that tracks global covid-19 daily cases (confirmed, deaths, recovered) along with the global temperature and population data (Country & State level) to help researchers analyze the covid-19 trend in certain location and the impact of temperature & population on covid-19.\n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "* Step 6: Project Alternative solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The project will be utilizing the data from Covid-19 Data Repository by the center for Systems Science and Engineering (CSSE) at Johns Hopkins University and global temperature data & population from Kaggle. The raw data will be extracted, cleansed, & saved in AWS S3 bucket. The designed ELT pipeline will then extract the data from S3, stage them in Redshift, and transform them into a set of dimentional tables & one fact table for researchers to develop insights with the covid-19 trend in various location. \n",
    "\n",
    "The end solution will include raw data saved in S3 & analytical data saved in Redshift for analysis purpose\n",
    "\n",
    "tools: AWS, Redshift, S3, Airflow (for future data pipeline task management)\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "Data Source:\n",
    "\n",
    "Global temperatures by state:\n",
    "    https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data?select=GlobalLandTemperaturesByState.csv \\\n",
    "    includes the average temperature by state & country from 1750 to 2013\n",
    "    \n",
    "Global covid 19 daily report:\n",
    "    https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports \\\n",
    "    includes the daily covid-19 cases (confirmed, deaths, recovered) by state & country from 2020-01-22 to as of today \n",
    "    \n",
    "Indian population state & district data:\n",
    "    https://www.kaggle.com/sirpunch/indian-census-data-with-geospatial-indexing?select=district+wise+population+for+year+2001+and+2011.csv \\\n",
    "    includes state, and district wise population for year 2001 and 2011\n",
    "\n",
    "China population by province data:\n",
    "    https://www.kaggle.com/quanncore/china-provinces-population \\\n",
    "    includes province level population data; Data has been collected from: http://population.city/ manually.\n",
    "\n",
    "US population by state data:\n",
    "    https://www.census.gov/programs-surveys/popest/technical-documentation/research/evaluation-estimates/2020-evaluation-estimates/2010s-state-detail.html \\\n",
    "    includes estimates of the Total Resident Population and Resident Population Age 18 Years and Older in US by state\n",
    " \n",
    "Data Caveats and assumptions:\n",
    "* The timestamp aspect of temperature & population data is not considered in this project\n",
    "* Population data contains only three major countries for this project for demostration purpose\n",
    "* Since there is no API for the temperature & population data, all four csv files are manually downloaded and uploaded to the project workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re \n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Set up all AWS configurations to connect to the redshift cluster in the next step\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET= config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "DWH_DB= config.get(\"DWH\",\"DB_NAME\")\n",
    "DWH_DB_USER= config.get(\"DWH\",\"DB_USER\")\n",
    "DWH_DB_PASSWORD= config.get(\"DWH\",\"DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DB_PORT\")\n",
    "\n",
    "DWH_ENDPOINT=config.get(\"DWH\",\"HOST\")\n",
    "DWH_ROLE_ARN=config.get(\"IAM_ROLE\",\"ARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Connect to Redshift Cluster\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the covid-19 daily data here--scrap the data from the source link\n",
    "\n",
    "# Store the url as a string scalar: url => str\n",
    "url = \"https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports\"\n",
    "\n",
    "# Issue request: r => requests.models.Response\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extract text: html_doc => str\n",
    "html_doc = r.text\n",
    "\n",
    "# Parse the HTML: soup => bs4.BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags => bs4.element.ResultSet\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Store a list of urls ending in .csv: urls => list\n",
    "urls = ['https://raw.githubusercontent.com'+re.sub('/blob', '', link.get('href')) \n",
    "        for link in a_tags  if '.csv' in link.get('href')]\n",
    "\n",
    "# Initialise an empty list the same length as the urls list: df_list => list\n",
    "df_list = [pd.DataFrame([None]) for i in range(len(urls))]\n",
    "\n",
    "# Store an empty list of dataframes: df_list => list\n",
    "\n",
    "df_list = [pd.read_csv(url, sep = ',') for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load all COVID-19 csv files into one dataframe\n",
    "df= pd.concat(df_list, axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read in Global Land Temperature by State csv as dataframe\n",
    "df_temp=pd.read_csv(\"GlobalLandTemperaturesByState.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read in all population data as dataframe\n",
    "df_china_base=pd.read_csv(\"china_provinces_population.csv\")\n",
    "df_us_base=pd.read_csv(\"sc-est2020-18+pop-res.csv\")\n",
    "df_india_base=df_india_base=pd.read_csv('district wise population for year 2001 and 2011.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://16b96ef06a90:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1db0b62128>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.\\\n",
    "# config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "# .enableHiveSupport().getOrCreate()\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean up Covid-19 Daily Report Data--handle data inconsistancy & duplicates & missing values etc.\n",
    "\n",
    "#one type of colunm names\n",
    "col_A=[\"FIPS\",\"Admin2\",\"Province_State\",\"Country_Region\",\"Last_Update\",\"Lat\",\"Long_\",\"Confirmed\",\"Deaths\",\"Recovered\"]\n",
    "\n",
    "#the other type of column names\n",
    "col_B=[\"FIPS\",\"Admin2\",\"Province/State\",\"Country/Region\",\"Last Update\",\"Latitude\", \"Longitude\",\"Confirmed\",\"Deaths\",\"Recovered\"]\n",
    "\n",
    "\n",
    "#Process the data saved in with column names as col_A: Drop the duplicates, rename the column names, update the timestamp field to proper format\n",
    "df_A=df[df['Country/Region'].isna()][col_A].copy()\n",
    "df_A.drop_duplicates(inplace=True)\n",
    "df_A.rename(columns = {'Lat':'Latitude', 'Long_':'Longitude', 'Province_State':'State','Country_Region':'Country'},inplace=True)\n",
    "df_A[\"Last_Update\"]=pd.to_datetime(df_A.Last_Update)\n",
    "df_A[\"Date\"]=df_A[\"Last_Update\"].dt.date\n",
    "\n",
    "#Process the data saved in with column names as col_B: Drop the duplicates, rename the column names, update the timestamp field to proper format\n",
    "df_B=df[~df['Country/Region'].isna()][col_B].copy()\n",
    "df_B.drop_duplicates(inplace=True)\n",
    "df_B.rename(columns={'Province/State':'State', 'Country/Region':'Country','Last Update':'Last_Update'}, inplace=True)\n",
    "df_B[\"Last_Update\"]=pd.to_datetime(df_B.Last_Update)\n",
    "df_B[\"Date\"]=df_B[\"Last_Update\"].dt.date\n",
    "\n",
    "df_covid=df_A.append(df_B)\n",
    "#Clean up the column formats\n",
    "df_covid['FIPS']=df_covid['FIPS'].astype('float64')\n",
    "df_covid['Latitude']=df_covid['Latitude'].astype('float64')\n",
    "df_covid['Longitude']=df_covid['Longitude'].astype('float64')\n",
    "df_covid['Confirmed']=df_covid['Confirmed'].astype('float64')\n",
    "df_covid['Deaths']=df_covid['Deaths'].astype('float64')\n",
    "df_covid['Recovered']=df_covid['Recovered'].astype('float64')\n",
    "df_covid['Year']=df_covid[\"Last_Update\"].dt.year\n",
    "df_covid['Month']=df_covid[\"Last_Update\"].dt.month\n",
    "\n",
    "#this step is used to create spark dataframe so that it will not cause any field type conflict\n",
    "df_covid['Admin2'].fillna(\"None\", inplace=True)\n",
    "df_covid['State'].fillna(\"None\", inplace=True)\n",
    "\n",
    "df_covid.drop_duplicates(inplace=True)\n",
    "df_covid.sort_values(by=[\"Year\",\"Month\",\"Country\",\"State\"], inplace=True)\n",
    "df_covid.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Admin2</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Last_Update</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2020-01-27 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-27</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2020-01-28 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-28</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2020-01-29 19:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-29</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2020-01-30 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-30</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Australia</td>\n",
       "      <td>2020-01-31 23:59:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIPS Admin2            State    Country         Last_Update  Latitude  \\\n",
       "0   NaN   None  New South Wales  Australia 2020-01-27 23:59:00       NaN   \n",
       "1   NaN   None  New South Wales  Australia 2020-01-28 23:00:00       NaN   \n",
       "2   NaN   None  New South Wales  Australia 2020-01-29 19:30:00       NaN   \n",
       "3   NaN   None  New South Wales  Australia 2020-01-30 16:00:00       NaN   \n",
       "4   NaN   None  New South Wales  Australia 2020-01-31 23:59:00       NaN   \n",
       "\n",
       "   Longitude  Confirmed  Deaths  Recovered        Date  Year  Month  \n",
       "0        NaN        4.0     NaN        NaN  2020-01-27  2020      1  \n",
       "1        NaN        4.0     NaN        NaN  2020-01-28  2020      1  \n",
       "2        NaN        4.0     NaN        NaN  2020-01-29  2020      1  \n",
       "3        NaN        4.0     NaN        2.0  2020-01-30  2020      1  \n",
       "4        NaN        4.0     NaN        2.0  2020-01-31  2020      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covid.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1855-05-01</td>\n",
       "      <td>25.544</td>\n",
       "      <td>1.171</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1855-06-01</td>\n",
       "      <td>24.228</td>\n",
       "      <td>1.103</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1855-07-01</td>\n",
       "      <td>24.371</td>\n",
       "      <td>1.044</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1855-08-01</td>\n",
       "      <td>25.427</td>\n",
       "      <td>1.073</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1855-09-01</td>\n",
       "      <td>25.675</td>\n",
       "      <td>1.014</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty State Country\n",
       "0  1855-05-01              25.544                          1.171  Acre  Brazil\n",
       "1  1855-06-01              24.228                          1.103  Acre  Brazil\n",
       "2  1855-07-01              24.371                          1.044  Acre  Brazil\n",
       "3  1855-08-01              25.427                          1.073  Acre  Brazil\n",
       "4  1855-09-01              25.675                          1.014  Acre  Brazil"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean up the temperature data\n",
    "\n",
    "# map the state to align with Covid-19 data\n",
    "# Cleaned up majority of the countries except Russia\n",
    "def map_state(x):\n",
    "    #US state update\n",
    "    if x==\"Georgia (State)\":\n",
    "        return \"Georgia\"\n",
    "    #China state update\n",
    "    elif  x==\"Ningxia Hui\":\n",
    "        return \"Ningxia\"\n",
    "    elif x==\"Xinjiang Uygur\":\n",
    "        return \"Xinjiang\"\n",
    "    elif x==\"Xizang\":\n",
    "        return \"Tibet\" \n",
    "    elif x==\"Nei Mongol\":\n",
    "        return \"Inner Mongolia\"\n",
    "    #India state update\n",
    "    elif x==\"Andaman And Nicobar\":\n",
    "        return \"Andaman and Nicobar Islands\"\n",
    "    elif x==\"Dadra And Nagar Haveli\":\n",
    "        return \"Dadra and Nagar Haveli and Daman and Diu\"\n",
    "    elif x==\"Daman And Diu\":\n",
    "        return \"Dadra and Nagar Haveli and Daman and Diu\"\n",
    "    elif x==\"Orissa\":\n",
    "        return \"Odisha\"\n",
    "    #Russia state update\n",
    "    elif x==\"Adygey\":\n",
    "        return \"Adygea Republic\"\n",
    "    elif x==\"Amur\":\n",
    "        return \"Amur Oblast\"\n",
    "    elif x==\"Altay\":\n",
    "        return \"Altai Republic\"\n",
    "    elif x==\"Arkhangel'Sk\":\n",
    "        return \"Arkhangelsk Oblast\"\n",
    "    elif x==\"Astrakhan'\":\n",
    "        return \"Astrakhan Oblast\"\n",
    "    elif x==\"Bashkortostan\":\n",
    "        return \"Bashkortostan Republic\"\n",
    "    elif x==\"Belgorod\":\n",
    "        return \"Belgorod Oblast\"\n",
    "    elif x==\"Bryansk\":\n",
    "        return \"Bryansk Oblast\"\n",
    "    elif x==\"Buryat\":\n",
    "        return \"Buryatia Republic\"\n",
    "    elif x==\"Chechnya\":\n",
    "        return \"Chechen Republic\"\n",
    "    elif x==\"Chelyabinsk\":\n",
    "        return \"Chelyabinsk Oblast\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df_temp['Country']=df_temp['Country'].apply(lambda x: 'US' if x == 'United States' else x)\n",
    "\n",
    "df_temp['State']=df_temp['State'].apply(lambda x: map_state(x))\n",
    "df_temp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>AverageTemperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Australian Capital Territory</td>\n",
       "      <td>11.581977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Australia</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>17.231321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Northern Territory</td>\n",
       "      <td>24.788343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>23.017097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>South Australia</td>\n",
       "      <td>19.143123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Country                         State  AverageTemperature\n",
       "0  Australia  Australian Capital Territory           11.581977\n",
       "1  Australia               New South Wales           17.231321\n",
       "2  Australia            Northern Territory           24.788343\n",
       "3  Australia                    Queensland           23.017097\n",
       "4  Australia               South Australia           19.143123"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the average termperature for each country & state as a reference table\n",
    "df_temp_grp=df_temp.groupby(['Country', 'State'])['AverageTemperature'].mean().to_frame().reset_index()\n",
    "\n",
    "df_temp_grp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Population</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>4921532</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>731158</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>7421401</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3030522</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>39368078</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Population Country\n",
       "0     Alabama     4921532      US\n",
       "1      Alaska      731158      US\n",
       "2     Arizona     7421401      US\n",
       "3    Arkansas     3030522      US\n",
       "4  California    39368078      US"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean up China population data\n",
    "df_china=df_china_base.copy()\n",
    "df_china['Country']='China'\n",
    "df_china.rename(columns={'PROVINCE NAME':'State', 'POPULATION':'Population'}, inplace=True)\n",
    "\n",
    "#Clean up US population data\n",
    "df_us=df_us_base[df_us_base['NAME'] !='United States'][['NAME', 'POPESTIMATE2020']]\n",
    "df_us['Country']='US'\n",
    "df_us.rename(columns={'NAME':'State', 'POPESTIMATE2020':'Population'}, inplace=True)\n",
    "\n",
    "#Clean up Indian population data\n",
    "def map_india_state(x):   \n",
    "    if x==\"Andaman & Nicobar Islands\":\n",
    "        return \"Andaman and Nicobar Islands\"\n",
    "    elif x==\"Dadra & Nagar Haveli\":\n",
    "        return \"Dadra and Nagar Haveli and Daman and Diu\"\n",
    "    elif x==\"Daman & Diu\":\n",
    "        return \"Dadra and Nagar Haveli and Daman and Diu\"\n",
    "    elif x==\"Jammu & Kashmir\":\n",
    "        return \"Jammu and Kashmir\"\n",
    "    elif x==\"Odisha (Orissa)\":\n",
    "        return \"Odisha\"\n",
    "    elif x==\"Puducherry (Pondicherry)\":\n",
    "        return \"Puducherry\"\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "df_india_base['State']=df_india_base['State'].apply(lambda x: map_india_state(x))\n",
    "df_india=df_india_base[['State','Population in 2011']].groupby(['State'])['Population in 2011'].sum().to_frame().reset_index()\n",
    "df_india['Country']='India'\n",
    "df_india.rename(columns={'Population in 2011':'Population'}, inplace=True)\n",
    "\n",
    "\n",
    "#Combine population data from all three countries \n",
    "df_population= pd.concat([df_us,df_india, df_china], axis=0, ignore_index=True, sort=False)\n",
    "df_population.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Save the raw data to S3\n",
    "Save the daily Covid-19 data in S3 bucket with partition by Year & Month\n",
    "Save the temperature data directly to a S3 bucket since the data is already grouped & in a relatively small size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save the temperature data\n",
    "sdf_temp = spark.createDataFrame(df_temp_grp)\n",
    "\n",
    "#Save to local workspace folder\n",
    "#sdf_temp.write.mode(\"overwrite\").parquet(\"AvgTemperatureByState\")\n",
    "\n",
    "#Save to S3\n",
    "sdf_temp.write.mode(\"overwrite\").parquet(\"s3a://udacity-capston/AvgTemperatureByState\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save the temperature data\n",
    "sdf_pop = spark.createDataFrame(df_population)\n",
    "# sdf_pop.printSchema()\n",
    "\n",
    "#Save to local workspace folder\n",
    "#sdf_pop.write.mode(\"overwrite\").parquet(\"PopulationByState\")\n",
    "\n",
    "#Save to S3\n",
    "sdf_temp.write.mode(\"overwrite\").parquet(\"s3a://udacity-capston/PopulationByState\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Save the Daily Covid data\n",
    "sdf_covid=spark.createDataFrame(df_covid)\n",
    "\n",
    "#Save to local workspace folder\n",
    "#sdf_covid.write.partitionBy(\"year\",\"month\").mode(\"overwrite\").parquet(\"DailyCovid\")\n",
    "\n",
    "#Save to S3 with year & month as partition\n",
    "sdf_covid.write.partitionBy(\"year\",\"month\").mode(\"overwrite\").parquet(\"s3a://udacity-capston/DailyCovid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FIPS: double (nullable = true)\n",
      " |-- Admin2: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Last_Update: timestamp (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Confirmed: double (nullable = true)\n",
      " |-- Deaths: double (nullable = true)\n",
      " |-- Recovered: double (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf_covid.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Below is the Entity Relationship Diagram:\n",
    "![ERD](ERD.JPG)\n",
    "The fact_covid the fact table that keeps track of the measurements such as the number of confirmed cases, deaths, and recovered.\n",
    "The dim_time and dim_location are served as dimensional tables to enrich the analysis around time & location. The temperature & population data are aggregated to the State and Country level as the additional attributes in dim_location table.\n",
    "\n",
    "The star schema is used for this project for its simple query design and fast aggregation for analysis purpose. As the purpose of the project is quite simple -- to analyze the covid-trend with the impact of timing, temperature, and population. Keeping all covid measurement data in one fact table and all other critical attributes in other dimensional tables makes most sense for this project.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "1. create all the analytical tables and staging tables\n",
    "2. Copy the data from S3 to staging tables on Redshift\n",
    "3. Transform the staging tables and load the data into each analytical table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS \"staging_covid\";\n",
    "CREATE TABLE IF NOT EXISTS \"staging_covid\" (\n",
    "    \"FIPS\" varchar ,\n",
    "    \"Admin2\" varchar,\n",
    "    \"State\" varchar,\n",
    "    \"Country\" varchar,\n",
    "    \"Last_update\" TIMESTAMP,\n",
    "    \"Latitude\" numeric,\n",
    "    \"Longitude\" numeric,\n",
    "    \"Confirmed\" numeric,\n",
    "    \"Deaths\" numeric,\n",
    "    \"Recovered\" numeric,\n",
    "    \"Date\" DATE, \n",
    "    \"Year\" int4,\n",
    "    \"Month\" integer\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS \"staging_temp\";\n",
    "CREATE TABLE IF NOT EXISTS  \"staging_temp\" (\n",
    "    \"Country\" varchar,\n",
    "    \"State\" varchar,\n",
    "    \"AverageTemperature\" numeric\n",
    "\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS \"staging_pop\";\n",
    "CREATE TABLE IF NOT EXISTS \"staging_pop\" (\n",
    "    \"Country\" varchar,\n",
    "    \"State\" varchar,\n",
    "    \"Population\" numeric\n",
    ");\n",
    "\n",
    "DROP TABLE IF EXISTS \"fact_covid\";\n",
    "CREATE TABLE IF NOT EXISTS \"fact_covid\" (\n",
    "    \"covid_id\" INTEGER IDENTITY (1, 1) primary key distkey,\n",
    "    \"Last_update\" timestamp NOT NULL sortkey ,\n",
    "    \"location_id\" integer,  \n",
    "    \"Confirmed\" numeric,\n",
    "    \"Deaths\" numeric,\n",
    "    \"Recovered\" numeric,\n",
    "    \"Date\" DATE NOT NULL, \n",
    "    \"Year\" integer NOT NULL,\n",
    "    \"Month\" integer NOT NULL\n",
    "); \n",
    "\n",
    "DROP TABLE IF EXISTS \"dim_location\";\n",
    "CREATE TABLE IF NOT EXISTS \"dim_location\" (\n",
    "    \"location_id\" INTEGER IDENTITY (1, 1) primary key sortkey,\n",
    "    \"State\" varchar,\n",
    "    \"Country\" varchar NOT NULL,\n",
    "    \"AverageTemperature\" numeric,\n",
    "    \"Population\" numeric\n",
    "\n",
    ")\n",
    "diststyle all;\n",
    "\n",
    "DROP TABLE IF EXISTS \"dim_time\";\n",
    "CREATE TABLE IF NOT EXISTS \"dim_time\" (\n",
    "    \"Last_update\" timestamp NOT NULL primary key sortkey,\n",
    "    \"day\" int NOT NULL,\n",
    "    \"week\" int NOT NULL,\n",
    "    \"month\" int NOT NULL,\n",
    "    \"year\" int NOT NULL, \n",
    "    \"weekday\" int\n",
    "\n",
    ")\n",
    "diststyle all;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Load values to tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#copy covid data from S3 to staging table\n",
    "qry_covid_stage = \"\"\"\n",
    "    COPY staging_covid FROM {}\n",
    "        CREDENTIALS 'aws_iam_role={}'\n",
    "        REGION '{}'\n",
    "        FORMAT AS PARQUET\n",
    "\"\"\".format(config.get('S3','COVID_DATA'), config.get('IAM_ROLE','ARN') ,config.get('REGION','REGION'))\n",
    "%sql $qry_covid_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#copy temperature data from S3 to staging table\n",
    "qry_temp_stage = \"\"\"\n",
    "    COPY staging_covid FROM {}\n",
    "        CREDENTIALS 'aws_iam_role={}'\n",
    "        REGION '{}'\n",
    "        FORMAT AS PARQUET\n",
    "\"\"\".format(config.get('S3','TEMP_DATA'), config.get('IAM_ROLE','ARN') ,config.get('REGION','REGION'))\n",
    "%sql $qry_temp_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#copy population data from S3 to staging table\n",
    "qry_pop_stage = \"\"\"\n",
    "    COPY staging_covid FROM {}\n",
    "        CREDENTIALS 'aws_iam_role={}'\n",
    "        REGION '{}'\n",
    "        FORMAT AS PARQUET\n",
    "\"\"\".format(config.get('S3','POP_DATA'), config.get('IAM_ROLE','ARN') ,config.get('REGION','REGION'))\n",
    "%sql $qry_pop_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into dim_location (\n",
    "    State,\n",
    "    Country,\n",
    "    AverageTemperature,\n",
    "    Population\n",
    "\n",
    ") select distinct \n",
    "    t1.State,\n",
    "    t1.Country,\n",
    "    t2.AverageTemperature,\n",
    "    t3.Population\n",
    "    from staging_covid t1 left join staging_temp t2 on t1.State=t2.State and t1.Country=t2.Country\n",
    "        left join staging_pop t3 on t1.State=t3.State and t1.Country=t3.Country\n",
    "    where t1.Country is not null and t1.Country <>''\n",
    "    order by t1.Country, t1.State\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into fact_covid (   \n",
    "    Last_update,\n",
    "    location_id,    \n",
    "    Confirmed,\n",
    "    Deaths,\n",
    "    Recovered,\n",
    "    Date, \n",
    "    Year,\n",
    "    Month ) \n",
    "    \n",
    "    select distinct\n",
    "        c.Last_update,\n",
    "        l.location_id,\n",
    "        sum(c.Confirmed) as Confirmed,\n",
    "        sum(c.Deaths) as Deaths,\n",
    "        sum(c.Recovered) as Recovered,\n",
    "        c.Date, \n",
    "        c.Year,\n",
    "        c.Month     \n",
    "\n",
    "    from staging_covid c \n",
    "    \n",
    "    left join dim_location l on c.State=l.State and c.Country=l.Country\n",
    "    group by c.Last_update, l.location_id, c.Date, c.Year, c.Month \n",
    "    order by c.Last_update, l.location_id\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into dim_time ( \n",
    "    Last_update timestamp NOT NULL primary key sortkey,\n",
    "    day,\n",
    "    week,\n",
    "    month,\n",
    "    year, \n",
    "    weekday) \n",
    "\n",
    "    select distinct \n",
    "        c.Last_update,\n",
    "        EXTRACT (DAY FROM c.Last_update) as day,\n",
    "        EXTRACT (WEEK FROM c.Last_update) as week, \n",
    "        EXTRACT (MONTH FROM c.Last_update) as month,\n",
    "        EXTRACT (YEAR FROM c.Last_update) as year,\n",
    "        EXTRACT (WEEKDAY FROM c.Last_update) as weekday\n",
    "    from staging_covid c\n",
    "    order by c.Last_update\n",
    "    ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------------+------------------+----------+\n",
      "|location_id|               State|            Country|AverageTemperature|Population|\n",
      "+-----------+--------------------+-------------------+------------------+----------+\n",
      "|          1|                None|         Azerbaijan|              null|      null|\n",
      "|          2|                None|        Afghanistan|              null|      null|\n",
      "|          3|                None|            Albania|              null|      null|\n",
      "|          4|                None|            Algeria|              null|      null|\n",
      "|          5|                None|            Andorra|              null|      null|\n",
      "|          6|                None|             Angola|              null|      null|\n",
      "|          7|                None|Antigua and Barbuda|              null|      null|\n",
      "|          8|                None|          Argentina|              null|      null|\n",
      "|          9|                None|            Armenia|              null|      null|\n",
      "|         10|                None|              Aruba|              null|      null|\n",
      "|         11|Australian Capita...|          Australia|11.581977261731979|      null|\n",
      "|         12|External territories|          Australia|              null|      null|\n",
      "|         13|From Diamond Prin...|          Australia|              null|      null|\n",
      "|         14|Jervis Bay Territory|          Australia|              null|      null|\n",
      "|         15|     New South Wales|          Australia| 17.23132123850991|      null|\n",
      "|         16|                None|          Australia|              null|      null|\n",
      "|         17|  Northern Territory|          Australia|24.788342750533108|      null|\n",
      "|         18|          Queensland|          Australia|23.017097368421073|      null|\n",
      "|         19|     South Australia|          Australia| 19.14312318840585|      null|\n",
      "|         20|            Tasmania|          Australia|10.861563134978217|      null|\n",
      "+-----------+--------------------+-------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the final results of the data warehouse table -- dim_location saved in the local workspace \n",
    "dim_location=spark.read.parquet(\"spark-warehouse/dim_location\")\n",
    "dim_location.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+-----+----+-------+\n",
      "|        Last_update|day|week|month|year|weekday|\n",
      "+-------------------+---+----+-----+----+-------+\n",
      "|2021-02-21 05:24:33| 21|   7|    2|2021|      1|\n",
      "|2021-02-22 05:24:21| 22|   8|    2|2021|      2|\n",
      "|2021-02-23 05:23:41| 23|   8|    2|2021|      3|\n",
      "|2021-02-24 05:29:16| 24|   8|    2|2021|      4|\n",
      "|2021-02-25 05:24:57| 25|   8|    2|2021|      5|\n",
      "|2021-02-26 05:22:40| 26|   8|    2|2021|      6|\n",
      "|2021-02-27 05:22:28| 27|   8|    2|2021|      7|\n",
      "|2021-02-28 05:22:20| 28|   8|    2|2021|      1|\n",
      "|2021-03-01 05:23:01|  1|   9|    3|2021|      2|\n",
      "|2021-03-02 05:23:30|  2|   9|    3|2021|      3|\n",
      "|2021-03-03 05:23:28|  3|   9|    3|2021|      4|\n",
      "|2021-03-04 05:24:24|  4|   9|    3|2021|      5|\n",
      "|2021-03-05 05:26:29|  5|   9|    3|2021|      6|\n",
      "|2021-03-06 04:23:41|  6|   9|    3|2021|      7|\n",
      "|2021-03-07 05:21:54|  7|   9|    3|2021|      1|\n",
      "|2020-10-31 04:24:44| 31|  44|   10|2020|      7|\n",
      "|2020-11-01 04:36:19|  1|  44|   11|2020|      1|\n",
      "|2020-11-02 05:25:04|  2|  45|   11|2020|      2|\n",
      "|2020-11-03 06:00:16|  3|  45|   11|2020|      3|\n",
      "|2020-11-04 05:25:09|  4|  45|   11|2020|      4|\n",
      "+-------------------+---+----+-----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the final results of the data warehouse table -- dim_time saved in the local workspace \n",
    "dim_time=spark.read.parquet(\"spark-warehouse/dim_time\")\n",
    "dim_time.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----------+---------+-------+---------+----------+----+-----+\n",
      "|covid_id|        Last_update|location_id|Confirmed| Deaths|Recovered|      Date|year|month|\n",
      "+--------+-------------------+-----------+---------+-------+---------+----------+----+-----+\n",
      "|  327322|2021-07-01 04:21:19|         64| 552937.0|15469.0| 513108.0|2021-07-01|2021|    7|\n",
      "|  328084|2021-07-02 04:21:47|         64| 554681.0|15491.0| 517245.0|2021-07-02|2021|    7|\n",
      "|  328846|2021-07-03 04:21:37|         64| 555831.0|15541.0| 519084.0|2021-07-03|2021|    7|\n",
      "|  329609|2021-07-04 04:21:38|         64| 556637.0|15576.0| 519084.0|2021-07-04|2021|    7|\n",
      "|  330372|2021-07-05 04:21:28|         64| 556667.0|15579.0| 519084.0|2021-07-05|2021|    7|\n",
      "|  331135|2021-07-06 04:21:32|         64| 556694.0|15587.0| 520959.0|2021-07-06|2021|    7|\n",
      "|  331899|2021-07-07 04:21:20|         64| 557708.0|15624.0| 520959.0|2021-07-07|2021|    7|\n",
      "|  327323|2021-07-01 04:21:19|         65| 396442.0| 8606.0| 249641.0|2021-07-01|2021|    7|\n",
      "|  328085|2021-07-02 04:21:47|         65| 396904.0| 8628.0| 262724.0|2021-07-02|2021|    7|\n",
      "|  328847|2021-07-03 04:21:37|         65| 398440.0| 8654.0| 262724.0|2021-07-03|2021|    7|\n",
      "|  329610|2021-07-04 04:21:38|         65| 400011.0| 8670.0| 262724.0|2021-07-04|2021|    7|\n",
      "|  330373|2021-07-05 04:21:28|         65| 401259.0| 8685.0| 266319.0|2021-07-05|2021|    7|\n",
      "|  331136|2021-07-06 04:21:32|         65| 401700.0| 8703.0| 266319.0|2021-07-06|2021|    7|\n",
      "|  331900|2021-07-07 04:21:20|         65| 402175.0| 8724.0| 266319.0|2021-07-07|2021|    7|\n",
      "|  327324|2021-07-01 04:21:19|         66|1284806.0|30711.0| 890344.0|2021-07-01|2021|    7|\n",
      "|  328086|2021-07-02 04:21:47|         66|1292791.0|30943.0| 901606.0|2021-07-02|2021|    7|\n",
      "|  328848|2021-07-03 04:21:37|         66|1296206.0|31128.0| 901606.0|2021-07-03|2021|    7|\n",
      "|  329611|2021-07-04 04:21:38|         66|1300485.0|31325.0| 901606.0|2021-07-04|2021|    7|\n",
      "|  330374|2021-07-05 04:21:28|         66|1302708.0|31418.0| 915019.0|2021-07-05|2021|    7|\n",
      "|  331137|2021-07-06 04:21:32|         66|1305082.0|31529.0| 915019.0|2021-07-06|2021|    7|\n",
      "+--------+-------------------+-----------+---------+-------+---------+----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Show the final results of the data warehouse table -- dim_time saved in the local workspace \n",
    "fact_covid=spark.read.parquet(\"spark-warehouse/fact_covid\")\n",
    "fact_covid.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* check if the result table is empty or not; expected result: count >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|332606|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the final results saved in local workspace\n",
    "fact_covid=spark.read.parquet(\"spark-warehouse/fact_covid\")\n",
    "fact_covid.createOrReplaceTempView(\"fact_covid\")\n",
    "spark.sql(\"select count(*) as count from fact_covid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "| 1028|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the final results saved in local workspace\n",
    "dim_location=spark.read.parquet(\"spark-warehouse/dim_location\")\n",
    "dim_location.createOrReplaceTempView(\"dim_location\")\n",
    "spark.sql(\"select count(*) as count from dim_location\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "| 3006|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read the final results saved in local workspace\n",
    "dim_time=spark.read.parquet(\"spark-warehouse/dim_time\")\n",
    "dim_time.createOrReplaceTempView(\"dim_time\")\n",
    "spark.sql(\"select count(*) as count from dim_time\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* check unique key in dim_location table; \n",
    "* the count of primary key (location_id); the distinct count of primary key (location_id); the distinct count of (country & state) combination\n",
    "* expected result: all three count fields should return the same value as 1028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+-------------------+\n",
      "|location_id_count|distinct_location_id_count|Country_State_count|\n",
      "+-----------------+--------------------------+-------------------+\n",
      "|             1028|                      1028|               1028|\n",
      "+-----------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check location count\n",
    "#check the location count--should be 1028 unique records\n",
    "spark.sql(\"\"\"select count(location_id) as location_id_count, \n",
    "            count(distinct location_id) as distinct_location_id_count,\n",
    "          count(distinct (Country,State)) as Country_State_count from dim_location\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Source/Count checks to ensure completeness\n",
    "* Check the dim_time table contains values in proper time range;\n",
    "* Expecked result: min timestamp: 2020-01-22 & max timestamp: 2021-07-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|             min_ts|             max_ts|\n",
      "+-------------------+-------------------+\n",
      "|2020-01-22 17:00:00|2021-07-07 04:21:20|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the final results saved in local workspace\n",
    "spark.sql(\"select min(Last_update) as min_ts, max(Last_update) as max_ts from dim_time\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Please see Data Dictionary.xlsx file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    \n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Clearly state the rationale for the choice of tools and technologies for the project. \\\n",
    "This project is built fully on AWS cloud service by utilizing S3 as the raw data storage and Redshift as the data warehouse platform. In term of the pricing with the storage option, AWS is relatively cheap comparing to other physical servers. Loading data from S3 and Redshift with the copy option makes the ETL process much simple and efficient. Redshift is also easy to scale up and out with its compute nodes, which will help when database grows. In addition, with AWS platform, there are also great built-in tools such as SageMaker for building machine learning models, QuickSight for data visualization, Apache Airflow for scheduling data pipelines etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Propose how often the data should be updated and why. \\\n",
    "Currently the data source for Covid-19 daily reports updates daily and new csv record is created with the data from the prior day with around 4000 rows. To keep the analytics up to date as the source, the data should be updated daily. Since it will be incremental load with one csv file everyday, the Airflow schedule job will be relatively quick and low maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The data was increased by 100x. \\\n",
    "If the data was increased by 100x, the S3 storage option can be utilized along with the Lifecycle policy. The older version of the raw data can be transferred to either S3 Standard IA or Glacier for archive purpose. As for the data warehouse in Redshift, either scaling up the compute nodes or adding more nodes can help with the huge increase of the data size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day. \\\n",
    "This can be done by setting up the scheduling data pipeline with Apache Airflow. Apache Airflow allows users to write DAGs in python and run the data pipeline along with the data validation on a schedule. It also provides an UI to track the data pipeline schedule performance, which will help the users to check if any step failed in the ETL process. The job can also be triggered manually after fixing the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* The database needed to be accessed by 100+ people. \\\n",
    "Depend on the different needs by the increase of audience, there are couple different options. For example, if the research group is specifically looking for the Covid-19 tracker for the U.S., the smaller data warehouse CUBE can be created with U.S. only data from the existing database. This will help the query performance and resource utilization. If the group of audience is doing the repetitive query for a dashboard view, the solution can be as the prior answer--materialized view and schedule SQL query run. The dashboard can be the destination for this group of audience for the checking the analysis results, instead of providing read access to the database directly. In addition, Redshift provides linear concurrency scaling for increase of concurrent users and it can boost throughput by 35 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 6: Project Alternative solution\n",
    "\n",
    "Run steps in command line:\n",
    "1. prep_data.py\n",
    "2. create_tables.py\n",
    "3. etl.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
